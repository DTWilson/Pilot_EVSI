---
title: "Untitled"
author: "Duncan T. Wilson"
date: "13/11/2019"
output:
  beamer_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE, eval = FALSE)
require(splines2)
require(ggplot2)
require(BB)
require(RColorBrewer)
cols <- brewer.pal(8, "Dark2")
```


# Introduction

## Last time
 
- External pilot trial followed by a definitive trial
- Both trials to be analysed using a hypothesis test
- Needed to choose type I and II error rates $\alpha_i, \beta_i, i=1,2$ at each stage
- Proposed to choose the rates which maximised expected utility, w.r.t. a prior distribution

## Maximising expected utility

- Rather than making a stop/go decision at the pilot stage through a hypothesis test, we could instead use MEU
- Given pilot data $x_1$, get the posterior $\mu | x_1$ and choose $\alpha_2, \beta_2$
- With this analysis in mind, how large should the pilot be?

## Simplifications

- Fix $\alpha_2 = 0.025$, so just need to choose $\beta_2$ or, equivalently, the definitive sample size $n_2$
- Assume the pilot data can be summarised in a single sufficient statistic

## MEU

Given pilot data $x_1$ from a pilot of sample size $n_1$:
$$
\max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(n_1, n_2. x_2)\big] 
$$

## MEU

Given a pilot of sample size $n_1$, average over the pilot data $x_1$:
$$
\mathbb{E}_{x_1 | n_1} \Big( \max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(n_1, n_2. x_2)\big] \Big)
$$

## MEU

Find optimal pilot sample size $n_1$:
$$
\max_{n_1} \Bigg[  ~\mathbb{E}_{x_1 | n_1} \Big( \max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(n_1, n_2. x_2)\big] \Big) \Bigg]
$$

# Computation

## Nested MC

$$
\max_{n_1} \Bigg[  ~\mathbb{E}_{x_1 | n_1} \Big( \max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(n_1, n_2. x_2)\big] \Big) \Bigg]
$$

- $\max_{n_1}$ - exhaustive / binary / other search
- $\mathbb{E}_{x_1 | n_1}$ - sample $N$ pilot data $x_1^{(i)} \sim p(x_1, | n_1)$
- $\max_{n_2}$ - exhaustive / binary / other search
- $\mathbb{E}_{\mu, x_2 | x_1, n_1}$ - sample $M$ posterior parameters $\mu^{(j)} \sim p(\mu | x_1, n_1)$

Extremely computationaly intensive.


## An alternative

Note that the inner optimisation problem maps pilot data $x_1$ to definitive sample size $n_2$.
$$
f(x_1): \mathcal{X_1} \rightarrow \mathbb{N}
$$
Ignoring optimising over $n_1$ for now, we can re-write our eearlier expression:
$$
\begin{aligned}
& \mathbb{E}_{x_1} \Big(\mathbb{E}_{\mu, x_2 | x_1}\big[u(n_1, \textcolor{red}{n_2 = f(x_1)}, x_2)\big] \Big) \\
=~ & \mathbb{E}_{\mu, x_1, x_2 } \Big(u(n_1, \textcolor{red}{f(x_1)}, x_2)\Big) \\
=~ & \mathbb{E}_{\mu, x_1} \Big(\mathbb{E}_{x_2 | \mu, x_1}\big[u(n_1, \textcolor{red}{f(x_1)}, x_2)\big] \Big) 
\end{aligned}
$$

## Single-level MC

We now have something we can estimate in a single-level Monte Carlo scheme:
$$
\mathbb{E}_{\mu, x_1} \Big(\mathbb{E}_{x_2 | \mu, x_1}\big[u(n_1, f(x_1), x_2)\big] \Big)
$$

- $\mathbb{E}_{\mu, x_1}$ - sample $N$  $(\mu^{(i)}, x_1^{(i)}) \sim p(\mu, x_1 | n_1) = p(x_1 | \mu, n_1)p(\mu)$

$$
\begin{aligned}
\mathbb{E}_{x_2 | \mu, x_1}\big[u(n_1, f(x_1), x_2)\big] =~ & Pr[x_2 > c_2 | \mu, n_2]~ u_1(\mu, n_1, n_2, x_2 > c_2) ~ + \\
& Pr[x_2 \leq c_2 | \mu, n_2] ~ u_1(\mu, n_1, n_2, x_2 \leq c_2)
\end{aligned}
$$

- $\mathbb{E}_{x_2 | \mu, x_1}$ - can be calculated exactly if the power function $Pr[x_2 > c_2 | \mu, n_2]$ is known

## Optimisation

All very well, but we do not know what the optimal decision rule $f(x_1)$ is. But note that

$$
\begin{gathered}
\mathbb{E}_{x_1 | n_1} \Big( \max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(n_1, n_2. x_2)\big] \Big) \\
\geq \\
\mathbb{E}_{\mu, x_1} \Big(\mathbb{E}_{x_2 | \mu, x_1, n_1}\big[u(n_1, f(x_1), x_2)\big] \Big), \text{ for all } f.
\end{gathered}
$$

$\Rightarrow$ if we can search over functions $f(x_1)$ and find that which maximises expected utility, this will coincide with the optimal decision rule.

## Splines

Spline: a piecewise polynomial defined by a set of \emph{knots} and by the degree of the polynomial pieces.

```{r}
set.seed(8736465)

# points to get the function value at
x <- seq(0,1,0.01)
# knots
knots <- seq(0.1,0.9,l=5)
# Basis function values at points x
# rows correspond to points
# columns are the individual basis functions (of which we have # knots + # degrees) 
bsMat <- bSpline(x, knots = knots, degree = 3) # using degree = 3 for cubic splines

# Given our basis functions, we can then build splines of the same degree and over the same knots
# as linear combinations of these.
as1 <- rnorm(5+3); as2 <- rnorm(5+3); as3 <- rnorm(5+3)
df <- data.frame(x = rep(x,3))
df$i <- as.factor(rep(1:3, each=length(x)))
df$y <- c(bsMat %*% as1, bsMat %*% as2, bsMat %*% as3)
df$hi <- ifelse(df$x <= knots[4] & df$x >= knots[3]-0.001, 1, 0)
```

```{r, include=T}
ggplot(df, aes(x,y,colour=i)) + geom_line() + 
  scale_x_continuous(breaks = knots) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  geom_vline(xintercept = knots, linetype=3)
```

```{r, include=T}
ggplot(df, aes(x,y,colour=i)) + geom_line(alpha=0.2) + 
  scale_x_continuous(breaks = knots) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  geom_line(data=df[df$hi == 1,], alpha=1) +
  geom_vline(xintercept = knots[3:4], linetype=3)
```

## B-splines

For a specific set of $k$ knots, there is a set of cubic splines which serve as a basis for the space of all cubic splines defined over the same knots. Denoting these so-called b-splines by $\phi_i(x), k=1,\ldots,k+4$, a function can thewn be written as
$$
f_{\mathbf{a}}(x) = \left\lfloor \sum_{i=1}^{k+4} a_i \phi_i(x) \right\rfloor,
$$

So, functions are then defined by a vector of coefficients $\mathbf{a} \in \mathbb{R}^{k+4}$.

## Example

```{r}
dfb <- data.frame(x=x, i=4, y=bsMat[,1])
for(n in 2:8){
  dfb <- rbind(dfb, data.frame(x=x, i=4+n-1, y=bsMat[,n]))
}

dfb$t <- "b"; df$t <- "p"
dfb$a <- sapply(as.numeric(dfb$i), function(x) as3[x-3]); df$a <- 1

dfb <- rbind(dfb, df[df$i ==3, c(1,2,3,5,6)])
dfb2 <- dfb
dfb2$y <- dfb2$y*dfb2$a
```

```{r, include=T}
ggplot(dfb[dfb$t == "p",], aes(x,y)) + geom_line(linetype = 2) + 
  geom_line(data=dfb[dfb$t == "b",], aes(group=i, colour=a)) +
  scale_color_viridis_c() +
  scale_x_continuous(breaks = knots) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())
```

## Example

```{r, include=T}
ggplot(dfb[dfb$t == "p",], aes(x,y)) + geom_line(linetype = 2) + 
  geom_line(data=dfb2[dfb2$t == "b",], aes(group=i, colour=a)) +
  scale_color_viridis_c() +
  scale_x_continuous(breaks = knots) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())
```

## Optimisation

An algorithm for estimating the expected utility of a pilot trial with sample size $n_1$:

- Choose a set of $k$ knots
- Generate a sample $(\textcolor{blue}{\mu^{(i)}, x_1^{(i)}}) \sim p(\mu, x_1 | n_1), i=1, \ldots, N$
- While not convereged:
    + select candidate $\textcolor{purple}{\mathbf{a}} \in \mathbb{R}^{k+4}$
    + calculate $\frac{1}{N} \sum_{i=1}^{N} \mathbb{E}_{x_2 | \textcolor{blue}{\mu^{(i)}, x_1^{(i)}}} \big[u(\textcolor{blue}{\mu^{(i)}}, n_1, f_{\textcolor{purple}{\mathbf{a}}}(\textcolor{blue}{x_1^{(i)}}), x_2)\big]$
    + check convergence
- Return maximum

Note that we use the same sample from $p(\mu, x_1 | n_1)$ at each iteration.

## OK-Diabetes

- Model, prior and utility parameterised as before
- Three approaches to compare:
    + Semi-analytic
    + Two-level nested MC
    + Proposed method
    

    
## Objective

When optimising, we are maximising
$$
\begin{aligned}
& \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}_{x_2 | \mu^{(i)}, x_1^{(i)}} \big[u(\mu^{(i)}, n_1, f_{\mathbf{a}}(x_1^{(i)}), x_2)\big] \\
=~& \frac{1}{N} \sum_{i=1}^{N} \Big[ pow(n)(1 - e^{-\rho (k_d\mu + k_n(n_1 + n_2))}) + (1-pow(n))(1 - e^{-\rho(k_n(n_1 + n_2) + k_c)}) \Big],
\end{aligned}
$$
where
$$
pow(n) = \left(z_{1-\alpha/2} - \frac{\mu}{\sqrt(2\sigma^2/n)} \right).
$$
In this case, we can get the derivative of this function with respect to $n_2$, and then with respect to the vector $\mathbf{a}$.

## OK-Diabetes

```{r}
get_ks <- function(d_bar, d_hat, n)
{
  k_d <- 1/(1 + d_hat - d_bar/n)
  k_n <- -k_d*d_bar/n
  k_c <- 1 - k_d - k_n
  
  return(c(k_d, k_n, k_c))
}

k <- get_ks(0.01, 0.2, 50)
rho <- 2; sd_0 <- 0.6; mu_0 <- 0; sig <- 1.5
np <- 30
```


```{r}
M <- 10^5
thetas <- rnorm(M, mu_0, sd_0)
xs <- rnorm(M, thetas, sqrt(2*sig^2/np))
```

```{r}
get_u_cond <- function(thetas, ns, np, k, rho, sig)
{
  # Expected utility of main trial - note we have assumed rho > 0

  # Get power
  pows <- 1-pnorm(qnorm(1-0.025) - thetas/sqrt(2*sig^2/ns))
  
  utils <- pows*(1-exp(-rho*(k[1]*thetas + k[2]*(ns+np)))) +
    (1-pows)*(1-exp(-rho*(k[2]*(ns+np) + k[3])))
  
  utils
}
```

```{r}
# Set up basis functions
knots <- seq(quantile(xs, c(0.01, 0.99))[1], quantile(xs, c(0.01, 0.99))[2], l = 10)
bsMat <- bSpline(xs, knots = knots, degree = 3, intercept = T) # using degree = 3 for cubic splines

get_u_mc <- function(z, thetas, bsMat, xs, np, k, rho, sig)
{
  # Get sample sizes from decision rule
  ns <- bsMat %*% as.numeric(z)#[1:14]

  # Penalty for negative sample size
  pen <- -sum(ns[ns < 0])*100000
  
  # Using the thetas associated with the x_1s and thus n_2s, average the conditional 
  # expected utilities
  u <- -mean(get_u_cond(thetas, ns, np, k, rho, sig))
  u + pen
}
```

Derivatives:
```{r}
get_grads_mc <- function(z, thetas, bsMat, xs, np, k, rho, sig)
{
  ns <- bsMat %*% as.numeric(z)
  
  f <- 1-pnorm(qnorm(1-0.025) - thetas/sqrt(2*sig^2/ns))
  fd <- -dnorm(qnorm(1-0.025) - thetas/sqrt(2*sig^2/ns))*(-thetas/(2*sqrt(2*ns*sig^2)))
  g1 <- 1 - exp(-rho*(k[1]*thetas + k[2]*(np + ns)))
  g1d <- rho*k[2]*exp(-rho*(k[1]*thetas + k[2]*(np + ns)))
  g2 <- 1 - exp(-rho*(k[2]*(np + ns) + k[3]))
  g2d <- rho*k[2]*exp(-rho*(k[2]*(np + ns) + k[3]))
  
  dfdn <- fd*(g1 - g2) + f*(g1d - g2d) + g2d
  
  -(t(dfdn) %*% bsMat)/length(ns)
}

# Check against numerical gradient
get_grads_mc(z, thetas[1], bsMat[1,], xs, np, k, rho, sig)
grad(get_u_mc, z, thetas=thetas[1], bsMat=bsMat[1,], np=np, k=k, rho=rho, sig=sig)
```

```{r}
z <- rep(1,14)
ptm <- proc.time()
opt2 <- suppressWarnings( BBoptim(z, get_u_mc, get_grads_mc,
                                  control = list(gtol=1.e-6, checkGrad=F),
                 thetas=thetas, bsMat=bsMat, np=np, k=k, rho=rho, sig=sig) )
proc.time() - ptm

z <- opt2$par; ns <- bsMat %*% as.numeric(z)
plot(xs[1:10^4], ns[1:10^4])
```


```{r}
exp_u <- function(x, np, k, rho, m, s, sig)
{
  n <- x[1];
  
  d <- qnorm(1-0.025)*sqrt(2*sig*sig/n)
  k_d <- k[1]; k_n <- k[2]; k_c <- k[3]
  
  sd_1 <- sqrt(1/(1/s^2 + n/(2*sig^2)))
  t <- -rho*k_d
  sig_x <- sqrt(s^2 + 2*sig^2/n)
  r <- (t * sd_1^2 *n)/(2*sig^2)

  -(  (1 - pnorm((d-m)/sig_x)) - exp(-rho*k_n*(n+np)) *
                                   exp(sd_1^2 *t*t/2) *
                                   exp(t*sd_1^2 *m/s^2) *
                                   exp(m*r + (sig_x^2*r*r/2)) *
                                   (1-pnorm((d-m)/sig_x - sig_x*r)) +
    pnorm((d-m)/sig_x) * (1 - exp(-rho*(k_n*(n+np) + k_c))) )
}

# Get the posterior variance
var_1 <- 1/( 1/(sd_0^2) + np/(2*sig^2) )
# Get posterior means corresponding to each pilot data sample
pms <- var_1*(mu_0/(sd_0^2) + np*xs/(2*sig^2))

ptm <- proc.time()
ns2 <- NULL; ds2 <- NULL
for(m in pms[1:M]){
  suppressWarnings( opt <- optim(c(1), exp_u, np=np, k=k, rho=rho, m=m, s=sqrt(var_1), sig=sig, method="L-BFGS-B",
             lower=c(0.001), upper=c(600)) )

  n <- opt$par
  ns2 <- c(ns2, n)
}
proc.time() - ptm
```

```{r}
us <- get_u_cond(thetas, ns, np, k, rho, sig); us2 <- get_u_cond(thetas, ns2, np, k, rho, sig)
mean(us); mean(us2); 
(log(1-mean(us)) - log(1-mean(us2)))/(rho*k[2])

df2 <- data.frame(x=rep(xs[1:10^3], 2), n=c(ns[1:10^3], ns2[1:10^3]), t=rep(c("Surrogate", "Semi-analytic"), each=10^3))
```

```{r, include=T}
ggplot(df2, aes(x, n, colour=t)) + geom_line() +
  scale_color_manual(values=cols[1:2], name="Method") +
  theme_minimal() +
  ylab("Sample size") + xlab("Pilot data")
```


## Extensions

- Optimising over $n_1$
- Adding a stop/go point
- Adding a decision rule mapping pilot data $x_1$ to optimal definitive type I error rate $\alpha_2$

`r knitr::knit_exit()`






## Introduction

"What is the principal distinction between Bayesian and classical statistics? It is that Bayesian statistics
is fundamentally boring. There is so little to do: just specify the model and the prior, and turn the
Bayesian handle" - Phil Dawid commenting on Lindley's The Philosophy of Staistics (2000)

Not just prior and model, but utility, and we can make any decision optimally.

Two major challenges in practice:
- specifying a utility (assuming model and prior are correct)
- doing the calculations

We will consider a specific problem in clinical trial design, and attempt to say something interesting about each of these challenges.

### Motivation

- Assessing effectiveness in complex intervention pilot trials to improve efficiency
- Testing not advised due to low power, an artefact of sharing the same endpoint and small pilot sample
- Not testing at all is never optimal, and probably never done in practice
- How do we choose optimal pilot error rates?

### BSDT

- Provides one way to define optimality
- (assume model and prior are given)
- utility and computations

## Part 1

### Problem

- Pilot and definitive trial, shared normal endpoint, known variance, unknown effect, two-arm etc.
- Normal conjgate prior on effect
- Make progression decision at each stage using a z-test of means
- Programme design defined by ns and cs at each stage

### Optimisation

- Search over the four parameters to maximise expected utility

### MEU theory

- Doing MEU is not a hueristic, but axiomatic
- Given some basic axioms about our preferences and how we want to behave (coherence), there exists a utility function s.t. preferences between distributions is encoded by its expectation
- Need to be able to find this specific utility, which means asking the right questions
- Contrast with ad-hoc "utilities", e.g. Thall

### Attributes

- First need to identify the things that we have preferences about
- Average change in outcome in the population
- Sample size at each stage
- "treatment costs" (known and deterministic)
- Attribute space

### Value

- Preferences in the attribute space under certainty
- Complete and transitive
- Looking for a value function which encodes preferences
- Introduce some structure, sum of single attribute functions (equivalent to assuming mutal preferential independance)
- More stucture, linearity in each single attribute function (implications)
- Need to dtermine three parameters, scaled, so two questions

### Utility

- Preference in the attribute space under uncertainty, so preferences over distributions
- All utility functions are value functions (just apply to degnerative dists), but not the converse
- Make an assumption that one attribute is utility independant of the other two (implications)
- Handy result, utility function must have exponential form, just one more parameter desribing risk attitude, one more question to elicit
- Note any additive utility implies all the above and risk-neutraility, and almost all utilities in the literature are additive

### Expected utility

- Condition on effect, closed form if we have a power function
- Integrate out effect, fast G-H quadrature since normal prior

### Application - OK-Diabetes

- Guess prior and utility parameters
- Compare optimal design with one constrained to not test in pilot
- sensitivity to prior and utility parameters

### Evaluation

- Optimal programmes for a range of utilities
- Testing in the pilot always optimal, but generally with high type I rate
- If we are sufficiently risk seeking, reduce to one stage design

### Extensions

- Internal pilot
- More complex designs and models, e.g. unknown variance or binary outcome
- More complex utilities, e.g. more attributes

## Part 2

### Problem

- If we have set up our utility, we can analyse the pilot through MEU
- Assume definitive type I set at default
- Now just need to choose the pilot sample size

### Nested MC algorithm

- Simple
- Computationally demanding
- Automating Bayesain inference without checks

### Surrogate decision rules

- Re-casting the problem as searching over rules mapping pilot data to definitive sample sizes
- Need a way to represent functions and search over them

### Splines

- Defined by their order and their knots
- Some example functions for a given set of knots
- Basis function representation

### Optimisation

- Add stop/go component
- incorportae a search over pilot n

### Application - OK-Diabetes

- Semi-analytic solution
- Nested MC
- Surrogate decision rules

### Limitations and alternatives

- EVSI (including regression based) focussed on few decisions
- GAM approach limited to one or two dimensions of pilot sample space
- Moment matching might be better, artificually restricting set of decisions

### Extensions

- Optimising defnitive trial alpha
- Internal pilots
- More complex designs and models, e.g. unknown variance or binary outcome
- More complex utilities, e.g. more attributes

## Summary

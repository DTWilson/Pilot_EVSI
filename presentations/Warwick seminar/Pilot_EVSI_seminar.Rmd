---
title: "Optimising external pilot trials of complex interventions using Bayesian statistical decision theory"
author: "Duncan T. Wilson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE, eval = T, 
                      fig.width=4, fig.height=2.5, out.width = "80%", fig.align = 'center')
require(splines2)
require(ggplot2)
require(BB)
require(numDeriv)
require(invgamma)
require(RColorBrewer)
require(fastGHQuad)
require(nloptr)
require(knitr)
require(reshape2)
cols <- brewer.pal(8, "Dark2")
```

## Introduction


"What is the principal distinction between Bayesian and classical statistics? It is that Bayesian statistics
is fundamentally **boring**. There is so little to do: *just specify the model and the prior, and turn the
Bayesian handle*" - Philip Dawid

## Introduction


"What is the principal distinction between Bayesian and classical statistics? It is that Bayesian statistics
is fundamentally **boring**. There is so little to do: *just specify the model and the prior and the utility, and turn the
Bayesian handle*" - Philip Dawid

We will consider how to apply the Bayesian decision-theoretic approach to problems of pilot trial design, considering first how we can define utility, and then how we do the handle turning.

## Motivation


```{r, include = T}
df <- data.frame(a = seq(0,1,0.001))
n <- 30

df$b <- apply(df, 1, function(x) 1 - power.t.test(n=n, delta=0.5, sd=1.5, sig.level = x[1], alternative = "o")$power)

p <- ggplot(df, aes(a,b)) + geom_line(size=1) +
  scale_color_manual(values=c("black", "grey40")) +
  coord_fixed() +
  xlab(expression(paste("Type I error rate, ", alpha))) +
  ylab(expression(paste("Type II error rate, ", beta))) +
  theme_minimal() +
  geom_point(data=df[df$a == 0.025,], colour=cols[1], size=3) +
  geom_point(data=df[df$a == 0.6,], colour=cols[3], size=3) +
  geom_point(data=df[df$a == 1,], colour=cols[2], size=3) +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  scale_x_continuous(breaks=seq(0,1,0.2)) +
  theme(panel.grid.major = element_line(colour = "grey50")) +
  theme(panel.grid.minor = element_line(colour = "grey80"))

p

# Assessing effectiveness in complex intervention pilot trials to improve efficiency
# Testing not advised due to low power, an artefact of sharing the same endpoint and small pilot sample
# Not testing at all is never optimal, and probably never done in practice - always have some implicit efficacy cut-off
# How do we choose optimal pilot error rates?
```

## A massive over-simplification

Suppose 

```{r}
p_n <- 0.8; p_a <- 1-p_n

sub <- df[df$a %in% c(0.025, 0.6, 1),]
sub$a2 <- 0.025; sub$b2 <- 0.1

sub$tp <- p_a*(1-sub$b)*(1-sub$b2)
sub$fp <- p_n*sub$a*sub$a2
sub$tn <- p_n*((1-sub$a) + sub$a*(1-sub$a2))
sub$fn <- p_a*(sub$b + (1-sub$b)*sub$b2)
sub$En <- 2*30 + (sub$a*p_n + (1-sub$b)*p_a)*2*190
```

## Bayesian Statistical Decision Theory

One way to define optimality is through Bayesian statistical decision theory. This assumes:

i) We can represent our *beliefs* through a probability distribution, $p$;

$$ 
A \text{ more likely than } B \Leftrightarrow p(A) > p(B)
$$

ii) We can represent our *preferences* through a utility function, $U$;

$$
A \text{ is preferable to } B \Leftrightarrow E[u(A)] > E[u(B)]
$$

It is axiomatic that this $p$ and $u$ exist (for an individual). But, how do we find them?

# Part 1 - Utilities

## Problem

Consider a two stage trial programme with an (external) pilot at stage $i = 1$ and a definitive trial at stage $i = 2$, with sample sizes $n_i$. We have a normally distributed primary outcome with known variance, and denote the sample mean difference by $x_i, i=1,2$.

A frequentist analysis:

- At stage 1, proceed to stage 2 if and only if $x_1 > d_1$;
- At stage 2, recommend the intervention over the control if and only if $x_2 > d_2$.

The design problem is then: optimise $(n_1, d_1, n_2, d_2)$.

## Attributes

To define our utility, we first need to choose a list of *attributes*: the things we have preferences about.

i) The average change outcome in the popullation
ii) The total sample size of the programme
iii) Difference in treatment costs between intervention and control

## Attributes [and assumtions]

To define our utility, we first need to choose a list of *attributes*: the things we have preferences about.

i) The average change outcome in the popullation [everyone will get the recommended treatment]
ii) The total sample size of the programme [no trial set-up costs]
iii) Difference in treatment costs between intervention and control [known and deterministic]

Denote these attributes by $d, n$, and $c$.

## Value

A useful intermediate step between attributes and utility is *value*: preferences under certainty.

$$
(d, n, c) \text{ is preffered  to } (d', n', c') \Leftrightarrow v(d, n, c) > v(d', n', c')
$$

It is helpful to identify structure in our preferences. For example, we could say that
$$
v(d, n, c) = k_d v_d(d) + k_n v_n(n) + k_c v_c(c),
$$
i.e. the overall value is a weighted sum of three single-attribute value functions. 

Q: When would this be reasonable to assume?

A: Under *pairwise preferential independance*

## Single-attribute value functions

Under pairwise preferential independance, elicitation is now much simpler.

$$
v_d(d) = d, ~~ v_n(n) = n, ~~ v_c(c) = c.
$$
Note the implicit assumptions:

- A change in attribute level from $d$ to $d + \Delta$ is independent of the starting level $d;
- Likewise for attribute n.

We are now left with determining the weights $k_d, k_n$ and $k_c$.

## Weights

We can scale everything to give $k_d + k_n + k_c = 1$. Then, we (at least) two questions:

i) Specify a change in outcome, $bar{d}$ that would justify increasing the total sample size from 0 to some value $n^*$:
$$
(d = 0; n = 0; c = 1) \sim (d = \bar{d}; n = n^*; c = 1).
$$
ii) Specify a change in outcome $\hat{d}$ that would justify switching from the current standard treatment to the intervention under study:
$$
(d = 0; n; c = 1) \sim (d = \hat{d}; n; c = 0).
$$

We can then solve for the weights.

## Utility

Finally, we need to use our value function to obtain a utility function, which needs to represent **preferences under uncertainty**.

Remarkably, it turns out that if we have an additive value function, then our utility must* take the form:

$$
u(d, n, c) =
\begin{cases}
1 - e^{-\rho v(d, n, c)}, &\rho > 0 \\
v(d, n, c), &\rho = 0 \\
-1 + e^{-\rho v(d, n, c)}, &\rho < 0 
\end{cases},
$$

* We also require that one attribute is *utility independant* of the other two.

Note:
- All utility functions are value functions, but not vice versa.
- Any additive utility function implies an additive value function and risk-neutrality.

## Attitude to risk

Utility elicitation is completed by determining $\rho \in \mathbb{R}$, which  represents our **attitude to risk**.

We ask for a value $d^*$ such that we would be indifferent between the following:

1. Obtaining $d^*$ with certainty;
2. A gamble which will result in $d_{min}$ with probability 0.5 and $d_{max}$ with probability 0.5.

That is, we find $d^*$ such that
$$
u(d^*, n, c) = 0:5 u(d_{min}, n, c) + 0:5 u(d_{max}, n, c),
$$
which we can solve for $\rho$. We will assume that we are risk-averse, giving $\rho > 0$.

## Expected utility

Let $G_i = 1$ if there is a positive test result at stage $i$, and $G_i = 0$ otherwise. Then the expected utility of a programme $z$, conditional on the true effect $\mu$, is
$$
\begin{split}
E_{G_1, G_2, | \mu, z}[u(\mu, G_1, G_2 ~|~ z)] =& Pr[G_1 = 1, G_2 = 1  \mid \mu]\left(1-e^{\rho(k_d\mu + k_n(n_1+n_2)}\right) + \\
& Pr[G_1 = 1,  G_2 = 0  \mid \mu] \left(1- e^{-\rho(k_n (n_1+n_2) + k_c)} \right) + \\
& Pr[G_1 = 0 \mid \mu] \left( 1-e^{-\rho(k_n n_1 + k_c)} \right)
\end{split}
$$
We can then integrate over the prior distribution for $\mu$:
$$
E[u(\mu, G_1, G_2 ~|~ z)] = \int E_{G_1, G_2, | \mu, z}[u(\mu, G_1, G_2 ~|~ z)] p(\mu) d\mu.
$$

```{r}
#Note: 
#- The probabilities are just power functions for the tests at each stage
#- If we have a normal prior on $\mu$, we can integrate numerically using quadrature
```

## Application: OK-Diabetes

- External pilot trial evaluating supported self-management for adults with learning disabilities and type II diabetes
- Sample size of $n_1 = 30$ based on a rule-of-thumb
- Asked by the funder to consider assessing the potential efficacy of the intervention, based on blood sugar levels
- Sample size increased to $n_1 = 56$ participants per arm, giving 0:82 power to detect a true mean reduction of 0.5\% using a one-sided test with a type I error rate of 0.2

## Application: OK-Diabetes

- Cost of new intervention:
$$
(d = 0; n; c = 1) \sim (\hat{d} = 0.3; n; c = 0)
$$

- Cost of sampling:
$$
(d = 0; n = 0; c = 1) \sim (\bar{d} = 0.005; n = 50; c = 1)
$$

- Attitude to risk:
$$
u(d^* = 0.19, n, c) = 0.5 u(d_{min} = 0, n, c) + 0:5 u(d_{max} = 0.5, n, c)
$$

- Prior on true treatment difference:
$$
\mu \sim Normal, ~~ \mathbb{E}[\mu] = 0, ~~ Pr[\mu > 0.5] = 0.2
$$

## Application: OK-Diabetes


```{r}
get_ks <- function(d_bar, d_hat, n)
{
  k_d <- 1/(1 + d_hat - d_bar/n)
  k_n <- -k_d*d_bar/n
  k_c <- 1 - k_d - k_n
  
  return(c(k_d, k_n, k_c))
}

# Condition on mu first
exp_u_mu <- function(mu, n1, d1, n2, d2, k, rho, mu_0, sd_0, sig)
{
  pow1 <- 1-pnorm(d1, mu, sqrt(2*sig^2/n1))
  pow2 <- 1-pnorm(d2, mu, sqrt(2*sig^2/n2))
  
  su <- 0 # set_up
  
  if(rho > 0){
    pow1*pow2*(1-exp(-rho*(k[1]*mu + k[2]*(n1+n2+su)))) +
    pow1*(1-pow2)*(1-exp(-rho*(k[2]*(n1+n2+su) + k[3]))) +
    (1-pow1)*(1-exp(-rho*(k[2]*n1 + k[3])))
  } else if(rho < 0) {
    pow1*pow2*(-1+exp(-rho*(k[1]*mu + k[2]*(n1+n2+su)))) +
    pow1*(1-pow2)*(-1+exp(-rho*(k[2]*(n1+n2+su) + k[3]))) +
    (1-pow1)*(-1+exp(-rho*(k[2]*n1 + k[3])))
  } else {
    pow1*pow2*(k[1]*mu + k[2]*(n1+n2+su)) +
    pow1*(1-pow2)*(k[2]*(n1+n2+su) + k[3]) +
    (1-pow1)*(k[2]*n1 + k[3])
  }
}

# For example,
d1 <- d2 <- 0.14
n1 <- n2 <- 20
mu_0 <- 0; sd_0 <- 0.6; sig <- 1.5
rho <- 2; 
d_bar <- 0.005; d_hat <- 0.3

k <- get_ks(d_bar, d_hat, n=50)

exp_u_mu(0.2, n1, d1, n2, d2, k, rho, mu_0, sd_0, sig)

# We are then left with integrating out the $\mu$:

# Set up the quadrature points and weights
rule <- gaussHermiteData(100)
rule$x <- rule$x*sqrt(2)*sd_0 + mu_0

exp_u_joint <- function(x, k, rho, mu_0, sd_0, sig, rule, pen = FALSE)
{
  n1 <- x[1]; d1 <- x[2]; n2 <- x[3]; d2 <- x[4]
  u <- ghQuad(f=exp_u_mu, rule=rule, n1=n1, d1=d1, n2=n2, d2=d2,
         k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig)/sqrt(pi)

  return(-u + pen*100*(n2<n1))
}

# For example,
exp_u_joint(c(5, 0.01, 30, 0.12), k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule)

mu_0 <- 0; sd_0 <- 0.6; sig <- 1.5; mu_1 <- 0.5
rho <- 2; 
d_bar <- 0.005; d_hat <- 0.3

k <- get_ks(d_bar, d_hat, n=50)
rule <- gaussHermiteData(100)
rule$x <- rule$x*sqrt(2)*sd_0 + mu_0


ptm <- proc.time()

x <- c(100, 0, 100, 0)
opt <- nloptr(x, exp_u_joint, 
                lb = c(30,-40,0,-40), ub= c(1000,40,1000,40),
                opt = list("algorithm"="NLOPT_GN_DIRECT", 
                           "xtol_rel"=1.0e-7,
                           "maxeval"=1000),
                k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule, pen=T)
  
  x <- opt$solution
  
  opt <- nloptr(x, exp_u_joint,
                lb = c(30,-40,0,-40), ub= c(1000,40,1000,40),
                opt = list("algorithm"="NLOPT_LN_SBPLX",
                           "xtol_rel"=1.0e-8,
                           "maxeval"=5000),
                k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule, pen=T)
proc.time() - ptm

x <- opt$solution

r1 <- c(x, 
         1 - pnorm(x[2]/sqrt(2*(sig^2)/x[1])),
         pnorm(x[2], mu_1, sqrt(2*sig^2/x[1])),
         1 - pnorm(x[4]/sqrt(2*(sig^2)/x[3])),
         pnorm(x[4], mu_1, sqrt(2*sig^2/x[3])),
        opt$objective)
r1

# Force a type I of 1 by constraining to low d_1
x <- c(100, -39, 100, 0)
opt <- nloptr(x, exp_u_joint, #eval_grad_f = exp_u_joint_grad,
                lb = c(30,-40,0,-40), ub= c(1000,-39,1000,40),
                opt = list("algorithm"="NLOPT_GN_DIRECT",  #"NLOPT_GD_STOGO",
                           "xtol_rel"=1.0e-7,
                           "maxeval"=1000),
                k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule, pen=F)
  
  x <- opt$solution
  
  opt <- nloptr(x, exp_u_joint,
                lb = c(30,-40,0,-40), ub= c(1000,-39,1000,40),
                opt = list("algorithm"="NLOPT_LN_SBPLX",
                           "xtol_rel"=1.0e-8,
                           "maxeval"=5000),
                k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule, pen=F)

x <- opt$solution

r2 <- c(x, 
         1 - pnorm(x[2]/sqrt(2*(sig^2)/x[1])),
         pnorm(x[2], mu_1, sqrt(2*sig^2/x[1])),
         1 - pnorm(x[4]/sqrt(2*(sig^2)/x[3])),
         pnorm(x[4], mu_1, sqrt(2*sig^2/x[3])),
        opt$objective)
r2
 
tab <- as.data.frame(rbind(r1, r2))

tab <- data.frame(pr = c("Unrestricted", "No pilot test"),
                  n_1=round(tab[,1]), n_2=round(tab[,3]),
                  a_1 = round(tab[,5], 2), p_1 = round(tab[,6], 3),
                  a_2 = round(tab[,7], 3), p_2 = round(tab[,8], 3),
                  u = round(tab[,9], 5)
                   )
colnames(tab) <- c("Problem", "$n_1$", "$n_2$", "$\\alpha_1$", "$\\beta_1$", "$\\alpha_2$", "$\\beta_2$", "Expected utility")
```

Compare the optimal designs under two strategies:

- Unrestricted: optimising over $\alpha_1, \beta_1, \alpha_2, \beta_2$
- No pilot test: optimising over $\alpha_2, \beta_2$ subject to $\alpha_1 = 1, n_1 = 30$

```{r, include = T}
kable(tab)
```

Note: the 0.00582 difference in expected utility approximately equals the cost of recruiting 66 participants.

## Evaluation

How do optimal designs vary with utility function parameters? Consider varying the attitude to risk, $\rho$:



```{r, include=T}
df <- readRDS("../../../Opt_pilot_ocs/data/opt_joint_np30.Rda")

df2 <- df[df$d_bar == 0.005 & df$d_hat == 0.3,]
df2$n1 <- df2$n1/500; df2$n2 <- df2$n2/500; 
df2 <- melt(df2, id.vars = c("rho", "d_bar", "d_hat"))
df2 <- cbind(df2[substr(df2$variable,2,2) == "1",], df2[substr(df2$variable,2,2) == "2",4:5])
names(df2)[4:7] <- c("t1", "v1", "t2", "v2")
df2 <- melt(df2, id.vars = c("rho", "d_bar", "d_hat", "t1", "t2"))
df2 <- df2[,c(1,2,3,4,6,7)]
df2$t1 <- substr(df2$t1,1,1)
df2$variable <- substr(df2$variable,2,2)
names(df2) <- c("rho", "d_bar", "d_hat", "var", "trial", "val")

ggplot(df2[df2$var %in% c("a", "b", "n"),], aes(rho, val, colour=trial, linetype=var))  + 
  geom_line() +
  theme_minimal() +
  scale_colour_manual(values=cols) +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  xlab(expression(paste("Attitude to risk, ", rho))) +
  ylab("Value") +
  labs(colour = "Stage", linetype = "Variable") +
  theme(panel.spacing = unit(2, "lines"), legend.position="bottom") +
  scale_linetype_manual(values=c(1,2,3), labels=c(expression(alpha), expression(1-beta), "n/500"))
```


## Extensions

- Internal pilot
- More complex designs and models, e.g. unknown variance or binary outcome
- More complex utilities, e.g. more attributes

# Part 2 - Computations

## Re-cap

We have seen Bayesian solution to a frequentist trial design problem:
 
- External pilot trial followed by a definitive trial
- Both trials to be analysed using a hypothesis test
- Needed to choose type I and II error rates $\alpha_i, \beta_i, i=1,2$ at each stage
- Proposed to choose the rates which maximised expected utility, w.r.t. a prior distribution

We have focussed so far on how to choose an appropriate utility function.

## A Bayesian approach

Rather than making a stop/go decision at the pilot stage through a frequentist hypothesis test, we could instead do a Bayesian decision-theoretic analysis:

- Given pilot data $x_1$, get the posterior $\mu | x_1$
- Choose $\alpha_2, \beta_2$ to maximise expected utility with respect to this posterior

With this analysis in mind, how large should the pilot trial be?

To simplify:

- Fix $\alpha_2 = 0.025$, so just need to choose $\beta_2$ or, equivalently, the definitive sample size $n_2$
- Assume the pilot data can be summarised in a single sufficient statistic (e.g. a sample mean)

## Notation re-cap

$\mu$: unknown parameter(s) of interest, with prior $p(\mu)$

$n_1, n_2$: sample size for pilot (1) and definitive (2) trials

$x_1, x_2$: data from the pilot (1) and definitive (2) trials

- Utility is a function of these:
$$
u(\mu, n_1, n_2. x_1, x_2)
$$

## Maximising expected utility

Given pilot data $x_1$ from a pilot of sample size $n_1$:
$$
\max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(\mu, n_1, n_2. x_2)\big] 
$$

## Maximising expected utility

Given a pilot of sample size $n_1$, average over the pilot data $x_1$:
$$
\mathbb{E}_{x_1 | n_1} \Big(~~~ \max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(\mu, n_1, n_2. x_2)\big] ~~~\Big)
$$

## Maximising expected utility

Find optimal pilot sample size $n_1$:
$$
\max_{n_1} \Bigg[~~~  ~\mathbb{E}_{x_1 | n_1} \Big(~~~ \max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(\mu, n_1, n_2. x_2)\big] ~~~\Big) ~~~\Bigg]
$$

## Two-level nested Monte Carlo

For now, forget about choosing pilot sample size. Given some $n_1$, how do we calculate

$$
\mathbb{E}_{x_1 | n_1} \Big(~~~ \max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(\mu, n_1, n_2. x_2)\big] ~~~\Big)
$$

We can:

- $\mathbb{E}_{x_1 | n_1}$ - sample $N$ pilot data $x_1^{(i)} \sim p(x_1, | n_1)$
- $\max_{n_2}$ - exhaustive / bisection / other search
- $\mathbb{E}_{\mu, x_2 | x_1, n_1}$ - sample $M$ posterior parameters $\mu^{(j)} \sim p(\mu | x_1, n_1)$

\textbf{Computational bottleneck:} for each outer pilot data sample $x_1^{(i)}$, need to generate a set of posterior samples (through MCMC).

## An alternative

Note that the inner optimisation problem maps pilot data $x_1$ to definitive sample size $n_2$:
$$
\begin{gathered}
f(x_1) = \operatorname*{argmax}_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1, n_1}\big[u(\mu, n_1, n_2. x_2)\big]\\
f: \mathcal{X_1} \rightarrow \mathbb{N}
\end{gathered}
$$
We can re-write our earlier expression using $f(x_1)$:
$$
\mathbb{E}_{x_1} \Big(\mathbb{E}_{\mu, x_2 | x_1}\big[u(\mu, n_1, \textcolor{red}{n_2 = f(x_1)}, x_2)\big] \Big)
$$
Now, with the "$\max$" removed, we can move around our expectations:

$$
\begin{aligned}
& \mathbb{E}_{x_1} \Big(\mathbb{E}_{\mu, x_2 | x_1}\big[u(\mu, n_1, f(x_1), x_2)\big] \Big) \\
=~ & \mathbb{E}_{\mu, x_1, x_2 } \Big(u(\mu, n_1, f(x_1), x_2)\Big) \\
=~ & \mathbb{E}_{\mu, x_1} \Big(\mathbb{E}_{x_2 | \mu, x_1}\big[u(\mu, n_1, f(x_1), x_2)\big] \Big) 
\end{aligned}
$$

## Single-level Monte Carlo

We now have something we can estimate in a single-level Monte Carlo scheme:
$$
\mathbb{E}_{\mu, x_1} \Big(\mathbb{E}_{x_2 | \mu, x_1}\big[u(\mu, n_1, f(x_1), x_2)\big] \Big)
$$

- $\mathbb{E}_{\mu, x_1}$ - sample $N$  $(\mu^{(i)}, x_1^{(i)}) \sim p(\mu, x_1 | n_1) = p(x_1 | \mu, n_1)p(\mu)$

$$
\begin{aligned}
\mathbb{E}_{x_2 | \mu, x_1}\big[u(\mu, n_1, f(x_1), x_2)\big] =~ & Pr[x_2 > c_2 | \mu, n_2]~ u(\mu, n_1, n_2, x_2 > c_2) ~ + \\
& Pr[x_2 \leq c_2 | \mu, n_2] ~ u(\mu, n_1, n_2, x_2 \leq c_2),
\end{aligned}
$$
where $c_2$ is the critical value in the definitive trial hypothesis test.

- $\mathbb{E}_{x_2 | \mu, x_1}$ - can be calculated exactly if the power function $Pr[x_2 > c_2 | \mu, n_2]$ is known

## Optimisation

\emph{But}, we do not know the optimal decision rule $f(x_1)$. Note that

$$
\begin{gathered}
\mathbb{E}_{x_1} \Big( \max_{n_2} ~\mathbb{E}_{\mu, x_2 | x_1}\big[u(\mu, n_1, \textcolor{red}{n_2}. x_2)\big] \Big) \\
\geq \\
\mathbb{E}_{\mu, x_1} \Big(\mathbb{E}_{x_2 | \mu, x_1, n_1}\big[u(\mu, n_1, \textcolor{red}{f(x_1)}, x_2)\big] \Big), \text{ for all } f.
\end{gathered}
$$

$\Rightarrow$ if we can search over functions $f(x_1)$ and find that which maximises expected utility, this will coincide with the optimal decision rule.


## Splines

Spline: a piecewise polynomial defined by a set of \emph{knots} and by the degree of the polynomial pieces.

```{r, eval=T}
set.seed(8736465)

# points to get the function value at
x <- seq(0,1,0.01)
# knots
#knots <- c(0.1, 0.2, 0.3, 0.6, 0.9)
knots <- seq(0.1,0.9,l=5)
# Basis function values at points x
# rows correspond to points
# columns are the individual basis functions (of which we have # knots + # degrees) 
bsMat <- bSpline(x, knots = knots, degree = 3) # using degree = 3 for cubic splines

# Given our basis functions, we can then build splines of the same degree and over the same knots
# as linear combinations of these.
as1 <- rnorm(5+3); as2 <- rnorm(5+3); as3 <- rnorm(5+3)
df <- data.frame(x = rep(x,3))
df$i <- as.factor(rep(1:3, each=length(x)))
df$y <- c(bsMat %*% as1, bsMat %*% as2, bsMat %*% as3)
df$hi <- ifelse(df$x <= knots[4] & df$x >= knots[3]-0.001, 1, 0)
```

```{r, include=T, eval=T}
ggplot(df, aes(x,y,colour=i)) + geom_line() + 
  scale_x_continuous(breaks = knots) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  geom_vline(xintercept = knots, linetype=3) +
    theme(panel.grid.minor = element_blank(),
        panel.grid = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.position = "none")
```

## Splines

Spline: a piecewise polynomial defined by a set of \emph{knots} and by the degree of the polynomial pieces.

```{r, include=T, eval=T}
ggplot(df, aes(x,y,colour=i)) + geom_line(alpha=0.3) + 
  scale_x_continuous(breaks = knots) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  geom_line(data=df[df$hi == 1,], alpha=1) +
  geom_vline(xintercept = knots[3:4], linetype=3) +
      theme(panel.grid.minor = element_blank(),
        panel.grid = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.position = "none")
```

## B-splines

For a specific set of $k$ knots, there is a set of cubic splines which serves as a basis for the space of all cubic splines defined over the same knots. 

Denoting these so-called b-splines by $\phi_i(x), k=1,\ldots,k+4$, a function can then be written as
$$
f_{\mathbf{a}}(x) = \sum_{i=1}^{k+4} a_i \phi_i(x) ,
$$

So, functions are then defined by a vector of coefficients $\mathbf{a} \in \mathbb{R}^{k+4}$.

## Example

```{r}
dfb <- data.frame(x=x, i=4, y=bsMat[,1])
for(n in 2:8){
  dfb <- rbind(dfb, data.frame(x=x, i=4+n-1, y=bsMat[,n]))
}

dfb$t <- "b"; df$t <- "p"
dfb$a <- sapply(as.numeric(dfb$i), function(x) as3[x-3]); df$a <- 1

dfb <- rbind(dfb, df[df$i ==3, c(1,2,3,5,6)])
dfb2 <- dfb
dfb2$y <- dfb2$y*dfb2$a
```

```{r, include=T}
ggplot(dfb[dfb$t == "p",], aes(x,y)) + geom_line(linetype = 2) + 
  geom_line(data=dfb[dfb$t == "b",], aes(group=i, colour=a)) +
  scale_color_viridis_c() +
  scale_x_continuous(breaks = knots) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

## Example

```{r, include=T}
ggplot(dfb[dfb$t == "p",], aes(x,y)) + geom_line(linetype = 2) + 
  geom_line(data=dfb2[dfb2$t == "b",], aes(group=i, colour=a)) +
  scale_color_viridis_c() +
  scale_x_continuous(breaks = knots) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

## Optimisation

An algorithm for estimating the expected utility of a pilot trial with sample size $n_1$:

- Choose a set of $k$ knots
- Generate a sample $\textcolor{blue}{(\mu^{(i)}, x_1^{(i)})} \sim p(\mu, x_1 | n_1), i=1, \ldots, N$
- While not convereged:
    + select candidate $\textcolor{purple}{\mathbf{a}} \in \mathbb{R}^{k+4}$
    + calculate $\frac{1}{N} \sum_{i=1}^{N} \mathbb{E}_{x_2 | \textcolor{blue}{\mu^{(i)}, x_1^{(i)}}} \big[u(\textcolor{blue}{\mu^{(i)}}, n_1, f_{\textcolor{purple}{\mathbf{a}}}(\textcolor{blue}{x_1^{(i)}}), x_2)\big]$
    + check convergence
- Return maximum

Note that we use the same sample from $p(\mu, x_1 | n_1)$ at each iteration.

## Application: OK-Diabetes

The function we are maximising is:
$$
\begin{aligned}
& \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}_{x_2 | \mu^{(i)}, x_1^{(i)}} \big[u(\mu^{(i)}, n_1, f_{\mathbf{a}}(x_1^{(i)}), x_2)\big] \\
=~& \frac{1}{N} \sum_{i=1}^{N} \Big[ pow(n)(1 - e^{-\rho (k_d\mu + k_n(n_1 + n_2))}) + \\ 
&(1-pow(n))(1 - e^{-\rho(k_n(n_1 + n_2) + k_c)}) \Big],
\end{aligned}
$$
where
$$
pow(n) = \Phi\left(z_{1-\alpha/2} - \frac{\mu}{\sqrt(2\sigma^2/n)} \right).
$$
We can get the derivative of this function with respect to the vector $\mathbf{a}$ - helps dramaticaly with optimisation.

## Application: OK-Diabetes

In this case (normal outcome, known variance, conjgate prior) we can get an exact expression for the expected utility of the definitive trial.

So, no need for MCMC sampling or numerical integration for the inner expectation $\rightarrow$ a more accurate answer.

## Application: OK-Diabetes

```{r}
get_ks <- function(d_bar, d_hat, n)
{
  k_d <- 1/(1 + d_hat - d_bar/n)
  k_n <- -k_d*d_bar/n
  k_c <- 1 - k_d - k_n
  
  return(c(k_d, k_n, k_c))
}

k <- get_ks(0.005, 0.3, 50)
rho <- 2; sd_0 <- 0.6; mu_0 <- 0; sig <- 1.5
np <- 30
```


```{r}
M <- 10^5
thetas <- rnorm(M, mu_0, sd_0)
xs <- rnorm(M, thetas, sqrt(2*sig^2/np))
```

```{r}
get_u_cond <- function(thetas, ns, np, k, rho, sig)
{
  # Expected utility of main trial - note we have assumed rho > 0

  # Get power
  pows <- 1-pnorm(qnorm(1-0.025) - thetas/sqrt(2*sig^2/ns))
  
  utils <- pows*(1-exp(-rho*(k[1]*thetas + k[2]*(ns+np)))) +
    (1-pows)*(1-exp(-rho*(k[2]*(ns+np) + k[3])))
  
  utils
}
```

```{r}
# Set up basis functions
knots <- seq(quantile(xs, c(0.01, 0.99))[1], quantile(xs, c(0.01, 0.99))[2], l = 10)
bsMat <- bSpline(xs, knots = knots, degree = 3, intercept = T) # using degree = 3 for cubic splines

get_u_mc <- function(z, thetas, bsMat, xs, np, k, rho, sig)
{
  # Get sample sizes from decision rule
  ns <- bsMat %*% as.numeric(z)

  # Penalty for negative sample size
  pen <- -sum(ns[ns < 0])*100000
  
  # Using the thetas associated with the x_1s and thus n_2s, average the conditional 
  # expected utilities
  u <- -mean(get_u_cond(thetas, ns, np, k, rho, sig))
  u + pen
}
```

```{r}
get_grads_mc <- function(z, thetas, bsMat, xs, np, k, rho, sig)
{
  ns <- bsMat %*% as.numeric(z)
  
  f <- 1-pnorm(qnorm(1-0.025) - thetas/sqrt(2*sig^2/ns))
  fd <- -dnorm(qnorm(1-0.025) - thetas/sqrt(2*sig^2/ns))*(-thetas/(2*sqrt(2*ns*sig^2)))
  g1 <- 1 - exp(-rho*(k[1]*thetas + k[2]*(np + ns)))
  g1d <- rho*k[2]*exp(-rho*(k[1]*thetas + k[2]*(np + ns)))
  g2 <- 1 - exp(-rho*(k[2]*(np + ns) + k[3]))
  g2d <- rho*k[2]*exp(-rho*(k[2]*(np + ns) + k[3]))
  
  dfdn <- fd*(g1 - g2) + f*(g1d - g2d) + g2d
  
  -(t(dfdn) %*% bsMat)/length(ns)
}

# Check against numerical gradient
z <- rep(1,14)
get_grads_mc(z, thetas[1], bsMat[1,], xs, np, k, rho, sig)
grad(get_u_mc, z, thetas=thetas[1], bsMat=bsMat[1,], np=np, k=k, rho=rho, sig=sig)
```

```{r}
z <- rep(1,14)
ptm <- proc.time()
opt2 <- suppressWarnings( BBoptim(z, get_u_mc, get_grads_mc,
                                  control = list(gtol=1.e-8, checkGrad=F),
                 thetas=thetas, bsMat=bsMat, np=np, k=k, rho=rho, sig=sig) )
proc.time() - ptm

z <- opt2$par; ns <- bsMat %*% as.numeric(z)
```


```{r}
exp_u <- function(x, np, k, rho, m, s, sig)
{
  n <- x[1];
  
  d <- qnorm(1-0.025)*sqrt(2*sig*sig/n)
  k_d <- k[1]; k_n <- k[2]; k_c <- k[3]
  
  sd_1 <- sqrt(1/(1/s^2 + n/(2*sig^2)))
  t <- -rho*k_d
  sig_x <- sqrt(s^2 + 2*sig^2/n)
  r <- (t * sd_1^2 *n)/(2*sig^2)

  -(  (1 - pnorm((d-m)/sig_x)) - exp(-rho*k_n*(n+np)) *
                                   exp(sd_1^2 *t*t/2) *
                                   exp(t*sd_1^2 *m/s^2) *
                                   exp(m*r + (sig_x^2*r*r/2)) *
                                   (1-pnorm((d-m)/sig_x - sig_x*r)) +
    pnorm((d-m)/sig_x) * (1 - exp(-rho*(k_n*(n+np) + k_c))) )
}

# Get the posterior variance
var_1 <- 1/( 1/(sd_0^2) + np/(2*sig^2) )
# Get posterior means corresponding to each pilot data sample
pms <- var_1*(mu_0/(sd_0^2) + np*xs/(2*sig^2))

ptm <- proc.time()
ns2 <- NULL; ds2 <- NULL; us2 <- NULL
for(m in pms[1:M]){
  suppressWarnings( opt <- optim(c(1), exp_u, np=np, k=k, rho=rho, m=m, s=sqrt(var_1), sig=sig, method="L-BFGS-B",
             lower=c(0.001), upper=c(600)) )
  ns2 <- c(ns2, opt$par); us2 <- c(us2, -opt$value)
}
proc.time() - ptm
```

```{r}
us <- get_u_cond(thetas, ns, np, k, rho, sig); us2 <- get_u_cond(thetas, ns2, np, k, rho, sig)
mean(us); mean(us2); 
(log(1-mean(us)) - log(1-mean(us2)))/(rho*k[2])

df2 <- data.frame(x=rep(xs[1:10^3], 2), n=c(ns[1:10^3], ns2[1:10^3]), t=rep(c("Surrogate", "Nested"), each=10^3))
```

Computation time: 9s (surrogate), 66s (nested)

```{r, include=T}
ggplot(df2, aes(x, n, colour=t)) + geom_line() +
  scale_color_manual(values=cols[1:2], name="Method") +
  theme_minimal() +
  ylab("Sample size") + xlab("Pilot data")
```


## Extensions

- Optimising over $n_1$
- Adding a stop/go point
- Adding a decision rule mapping pilot data $x_1$ to optimal definitive type I error rate $\alpha_2$
- Allowing for unknwon variance

## Unknown variance

Suppose we don't know the variance of the outcome, and have a prior on that as well as the mean.

- The pilot data can now be summarised by a 2D sufficient statistic, the sample mean $x_1$ and the sample variance $y_1$
- Now need to map this 2D space to choice of definitive trial sample size

We can use tensor product splines:

$$
\sum_{i=1}^{k+4} \sum_{j=1}^{k+4} a_{i,j} \phi_i(x_1) \psi_j(y_1)
$$

- Still defined by vector $\mathbf{a}$, but now $\mathbf{a} \in \mathbb{R}^{(k+4)^2}$
- In our example with $k = 10$, solution space dimensions go from 14 to 196
- Impact is managable \emph{if} we have derivatives

## Unknown variance

```{r}
get_u_cond <- function(thetas, ns, np, k, rho)
{
  # Expected utility of main trial - note we have assumed rho > 0
  sig <- sqrt(thetas[,1]); ms <- thetas[,2]
  
  pows <- 1-pnorm(qnorm(1-0.025) - ms/sqrt(2*sig^2/ns))
  
  utils <- pows*(1-exp(-rho*(k[1]*ms + k[2]*(ns+np)))) +
    (1-pows)*(1-exp(-rho*(k[2]*(ns+np) + k[3])))
  
  utils
}
```

```{r}
k <- get_ks(0.01, 0.2, 50)
rho <- 2; sd_0 <- 0.6; mu_0 <- 0; sig <- 1.5
np <- 30

M <- 10^5

thetas <- cbind(rinvgamma(M, shape=10, rate=1.5*(10-1)), rnorm(M, 0, 0.6))

x1s <- sqrt(thetas[,1]*rchisq(M, df=2*np-1)/(2*np-1))
x2s <- rnorm(M, thetas[,2], sqrt(2*thetas[,1]/np))

# Set up basis functions
knots1 <- seq(quantile(x1s, c(0.01, 0.99))[1], quantile(x1s, c(0.01, 0.99))[2], l = 10)
bsMat1 <- bSpline(x1s, knots = knots1, degree = 3, intercept = T) # using degree = 3 for cubic splines

knots2 <- seq(quantile(x2s, c(0.01, 0.99))[1], quantile(x2s, c(0.01, 0.99))[2], l = 10)
bsMat2 <- bSpline(x2s, knots = knots2, degree = 3, intercept = T) # using degree = 3 for cubic splines

bsMat <- NULL
for(i in 1:ncol(bsMat2)){
  bsMat <- cbind(bsMat, bsMat1*bsMat2[,i])
}

get_u_mc <- function(z, thetas, bsMat, np, k, rho)
{
  ns <- bsMat %*% as.numeric(z)
  pen <- -sum(ns[ns <= 0])*10^6
  # Using the thetas associated with the x_1s and thus n_2s, average the conditional 
  # expected utilities
  u <- -mean(get_u_cond(thetas, ns, np, k, rho), na.rm = T)
  u + pen
}
```

```{r}
get_grads_mc <- function(z, thetas, bsMat, np, k, rho)
{
  ns <- bsMat %*% as.numeric(z)
  sig <- sqrt(thetas[,1]); ms <- thetas[,2] 
  
  f <- 1-pnorm(qnorm(1-0.025) - ms/sqrt(2*sig^2/ns))
  fd <- -dnorm(qnorm(1-0.025) - ms/sqrt(2*sig^2/ns))*(-ms/(2*sqrt(2*ns*sig^2)))
  g1 <- 1 - exp(-rho*(k[1]*ms + k[2]*(np + ns)))
  g1d <- rho*k[2]*exp(-rho*(k[1]*ms + k[2]*(np + ns)))
  g2 <- 1 - exp(-rho*(k[2]*(np + ns) + k[3]))
  g2d <- rho*k[2]*exp(-rho*(k[2]*(np + ns) + k[3]))
  
  dfdn <- fd*(g1 - g2) + f*(g1d - g2d) + g2d
  
  -(t(dfdn) %*% bsMat)/length(ns)
}

# Check against numerical gradient
z <- rep(10,14^2)
gs1 <- get_grads_mc(z, thetas[1:2,], bsMat[1:2,], np, k, rho)
gs2 <- grad(get_u_mc, z, thetas=thetas[1:2,], bsMat=bsMat[1:2,], np=np, k=k, rho=rho)
plot(gs1, gs2)
```

```{r}
z <- rep(10,14^2)

ptm <- proc.time()
opt2 <- suppressWarnings( BBoptim(par=z, fn=get_u_mc, gr=get_grads_mc,
                                  control = list(gtol=1.e-8, checkGrad=F),
                 thetas=thetas, bsMat=bsMat, np=np, k=k, rho=rho) )
proc.time() - ptm

z <- opt2$par
ns <- (bsMat %*% as.numeric(z))

df <- data.frame(n=ns, x1=x1s, x2=x2s)
```

Computation time: 79s

```{r, include = T}
ggplot(df[1:10^4,], aes(x1, x2, colour=n)) + geom_point(alpha=0.3) +
  theme_minimal() +
  scale_color_viridis_c() +
  xlab("Pilot sample SD") + ylab("Pilot sample mean)")
```


## Summary

We can now quickly calculate and maximise the expected utility of pilot trials, making a Bayesian decision-theoretic approach to pilot design and analysis feasible.

\emph{But}:

- Need only 1 or 2 dimension sufficient statistic
- If 2 dimensions, need derivatives of objective function
- Need an analyticpower function for the definitive trial
- How can we be sure of converegnce to true global optimum?
